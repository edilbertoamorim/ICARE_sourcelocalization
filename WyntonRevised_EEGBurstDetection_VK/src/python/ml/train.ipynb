{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "jupyter nbconvert models.ipynb --to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import losswise\n",
    "from prettytable import PrettyTable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import importlib\n",
    "import pickle\n",
    "\n",
    "from datasets import BurstDataset, ShuffledBatchSequentialSampler, FakeBurstDataset\n",
    "from prep_dataset import BurstDatasetStandardizer\n",
    "from models import Encoder, Decoder\n",
    "from train_functions import *\n",
    "from eval_functions import plot_autoencoding, autoencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_saved_epoch(save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        # the first epoch which is run is epoch 1\n",
    "        return 0\n",
    "    saved_encs = os.listdir(save_dir)\n",
    "    max_enc_epoch = 0\n",
    "    max_dec_epoch = 0\n",
    "    for saved_enc in saved_encs:\n",
    "        if 'epoch' in saved_enc:\n",
    "            epoch_num = int(saved_enc.split('_')[0][5:])\n",
    "            if 'enc' in saved_enc:\n",
    "                max_enc_epoch = max(epoch_num, max_enc_epoch)\n",
    "            elif 'dec' in saved_enc:\n",
    "                max_dec_epoch = max(epoch_num, max_dec_epoch)\n",
    "    max_epoch = min(max_enc_epoch, max_dec_epoch)\n",
    "    return max_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wrapper(min_burst_secs, max_burst_secs, min_episode_mins, max_episode_mins, \n",
    "                  pad_length, batch_by_len, robust_scale, downsample_factor, data_dir, max_num_patients, \n",
    "                  max_num_bursts_per_episode, train_split, dev_split, \n",
    "                 hidden_size, input_size, bidirectional, num_layers, extra_input_dim, \n",
    "                  batch_size, num_epochs, lr, weight_decay, teacher_forcing_slope, train_reversed,\n",
    "                  save_dir, use_losswise, run_tag, config_name):\n",
    "    \n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    \n",
    "    # initialize model\n",
    "    encoder = Encoder(input_size, hidden_size, bidirectional=bidirectional, num_layers=num_layers)\n",
    "    decoder = Decoder(hidden_size, input_size, extra_input_dim=extra_input_dim, encoder_bidirectional=bidirectional, \n",
    "                      num_layers=num_layers)\n",
    "    if torch.cuda.is_available():\n",
    "        encoder = encoder.cuda()\n",
    "        decoder = decoder.cuda()\n",
    "     \n",
    "    # load saved model/initialize dataset\n",
    "    max_saved_epoch = get_max_saved_epoch(save_dir)\n",
    "    start_epoch_num = 1\n",
    "    if max_saved_epoch > 10:\n",
    "        # if we've saved a decent amount; otherwise, just rerun it. \n",
    "        print('loading saved models and datasets...')\n",
    "        encoder.load_state_dict(torch.load(os.path.join(save_dir, 'epoch{}_enc.pkl'.format(max_saved_epoch))))\n",
    "        decoder.load_state_dict(torch.load(os.path.join(save_dir, 'epoch{}_dec.pkl'.format(max_saved_epoch))))\n",
    "        (train_dataset, dev_dataset, test_dataset) = pickle.load(open(os.path.join(save_dir, 'datasets.pkl')))\n",
    "        start_epoch_num = max_saved_epoch + 1\n",
    "    else:\n",
    "        # initialize datasets\n",
    "        print('initializing datasets...')\n",
    "        dataset = BurstDataset(data_dir, sort_len=False)\n",
    "        dataset.init_dataset(pad_length, min_burst_secs=min_burst_secs, max_burst_secs=max_burst_secs, \n",
    "                           min_episode_mins=min_episode_mins, max_episode_mins=max_episode_mins, \n",
    "                            downsample_factor=downsample_factor, max_num_patients=max_num_patients, \n",
    "                            max_num_bursts_per_episode=max_num_bursts_per_episode)\n",
    "        train_dataset, dev_dataset, test_dataset = dataset.split(train_split, dev_split,\n",
    "                                                                 split_sort_len=batch_by_len)\n",
    "        standardizer = BurstDatasetStandardizer()\n",
    "        print('scaling datasets...')\n",
    "        standardizer.fit(train_dataset)\n",
    "        standardizer.transform(train_dataset, robust_scale)\n",
    "        standardizer.transform(dev_dataset, robust_scale)\n",
    "        standardizer.transform(test_dataset, robust_scale)\n",
    "\n",
    "    # get list of all the params\n",
    "    params_dict = {\n",
    "        # dataset filtering\n",
    "        'min_burst_secs':min_burst_secs, 'max_burst_secs':max_burst_secs, \n",
    "        'min_episode_mins':min_episode_mins, 'max_episode_mins':max_episode_mins, \n",
    "        # dataset size\n",
    "        'max_num_patients':max_num_patients, 'max_num_bursts_per_episode':max_num_bursts_per_episode,\n",
    "        'len(train_data)':len(train_dataset), \n",
    "        'train split':train_split, 'dev split':dev_split, \n",
    "        # dataset other\n",
    "        'pad_length': pad_length, \n",
    "        'robust_scale':robust_scale,\n",
    "        'downsample_factor':downsample_factor,\n",
    "        # model params\n",
    "        'hidden_size': hidden_size, 'bidirectional':bidirectional, 'num_layers':num_layers, \n",
    "        'extra_input_dim':extra_input_dim, \n",
    "        # training params\n",
    "        'batch_by_len': batch_by_len, 'batch_size':batch_size, 'num_epochs': num_epochs, \n",
    "        'lr': lr, 'weight decay': weight_decay, \n",
    "        'teacher_forcing_slope': teacher_forcing_slope, 'train reversed':train_reversed, \n",
    "        'save dir': save_dir, 'config_name':config_name, 'start_epoch_num':start_epoch_num}\n",
    "    if save_dir is not None:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        if max_saved_epoch <= 10:\n",
    "            pickle.dump(params_dict, open(os.path.join(save_dir, \"params_dict.pkl\"), \"w\"))\n",
    "            pickle.dump((train_dataset, dev_dataset, test_dataset), open(os.path.join(save_dir, \"datasets.pkl\"), \"w\"))\n",
    "            pickle.dump(standardizer, open(os.path.join(save_dir, \"standardizer.pkl\"), \"w\"))\n",
    "    # set up losswise\n",
    "    if use_losswise:\n",
    "        losswise.set_api_key('W2TAMB3SZ') # api_key for \"coma-eeg\"\n",
    "        session = losswise.Session(tag=run_tag, max_iter=num_epochs,\n",
    "                                   params=params_dict)\n",
    "        losswise_graph = session.graph('loss', kind='min')\n",
    "    else:\n",
    "        losswise_graph = None\n",
    "    try:\n",
    "        train_model(train_dataset, dev_dataset, test_dataset, encoder, decoder, save_dir,\n",
    "                num_epochs=num_epochs, start_epoch_num=start_epoch_num,\n",
    "                batch_size=batch_size, lr=lr, weight_decay=weight_decay, \n",
    "                teacher_forcing_slope=teacher_forcing_slope, train_reversed=train_reversed, batch_by_len=batch_by_len, \n",
    "                losswise_graph=losswise_graph, params_dict=params_dict)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    if use_losswise:\n",
    "        session.done()\n",
    "    \n",
    "    return train_dataset, dev_dataset, test_dataset, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ipykernel' in sys.argv[0]:\n",
    "    # running in nb\n",
    "    config_name = 'configtest'\n",
    "else:\n",
    "    if len(sys.argv) < 2:\n",
    "        print('must run with argument indicating config number')\n",
    "    config_name = sys.argv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = importlib.import_module('configs.{}'.format(config_name))\n",
    "globals().update(\n",
    "    {k: v for (k, v) in config.__dict__.items() if not k.startswith('_')\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = os.path.join(SAVE_DIR, config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, dev_dataset, test_dataset, encoder, decoder = train_wrapper(min_burst_secs=MIN_BURST_SECS, max_burst_secs=MAX_BURST_SECS, \n",
    "              min_episode_mins=MIN_EPISODE_MINS, max_episode_mins=MAX_EPISODE_MINS, \n",
    "              pad_length=PAD_LENGTH, batch_by_len=BATCH_BY_LEN, robust_scale=robust_scale, \n",
    "              downsample_factor=downsample_factor, data_dir=DATA_DIR, \n",
    "              max_num_patients=MAX_NUM_PATIENTS, max_num_bursts_per_episode=MAX_NUM_BURSTS_PER_EPISODE,\n",
    "              train_split=train_split, dev_split=dev_split, \n",
    "              hidden_size=HIDDEN_SIZE, input_size=INPUT_SIZE, bidirectional=BIDIRECTIONAL, \n",
    "              num_layers=NUM_LAYERS, extra_input_dim=EXTRA_INPUT_DIM, batch_size=BATCH_SIZE, \n",
    "              num_epochs=NUM_EPOCHS, lr=LR, weight_decay=WEIGHT_DECAY, \n",
    "              teacher_forcing_slope=TEACHER_FORCING_SLOPE, train_reversed=TRAIN_REVERSED,\n",
    "              save_dir=SAVE_DIR, use_losswise=USE_LOSSWISE, run_tag=RUN_TAG, config_name=config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample = train_dataset[8]\n",
    "mse = plot_autoencoding(sample, encoder, decoder, toss_encoder_output=False, reverse=TRAIN_REVERSED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
