{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import losswise\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from datasets import BurstDataset, ShuffledBatchSequentialSampler, FakeBurstDataset\n",
    "from prep_dataset import BurstDatasetStandardizer\n",
    "from models import Encoder, Decoder\n",
    "from eval_functions import plot_autoencoding, autoencode, encode\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "from readers.patient_info import PatientInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = 'saved_encs/config18/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = pickle.load(open(os.path.join(SAVE_DIR, 'params_dict.pkl')))\n",
    "(train_dataset, dev_dataset, test_dataset) = pickle.load(open(os.path.join(SAVE_DIR, 'datasets.pkl')))\n",
    "standardizer = pickle.load(open(os.path.join(SAVE_DIR, 'standardizer.pkl')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = params_dict['hidden_size']\n",
    "INPUT_SIZE = 1 # This CANNOT be changed! \n",
    "BIDIRECTIONAL = params_dict['bidirectional']\n",
    "NUM_LAYERS = params_dict['num_layers']\n",
    "EXTRA_INPUT_DIM = params_dict['extra_input_dim']\n",
    "\n",
    "encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, bidirectional=BIDIRECTIONAL, num_layers=NUM_LAYERS)\n",
    "decoder = Decoder(HIDDEN_SIZE, INPUT_SIZE, extra_input_dim=EXTRA_INPUT_DIM, encoder_bidirectional=BIDIRECTIONAL, \n",
    "                  num_layers=NUM_LAYERS)\n",
    "if torch.cuda.is_available():\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = params_dict['num_epochs']\n",
    "# 278 is the best epoch for config18\n",
    "use_epoch = 278 \n",
    "encoder.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'epoch{}_enc.pkl'.format(use_epoch))))\n",
    "decoder.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'epoch{}_dec.pkl'.format(use_epoch))))\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the autoencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_REVERSED = params_dict['train reversed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample_factor = params_dict['downsample_factor']\n",
    "robust = params_dict['robust_scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "dataset = test_dataset\n",
    "for i in range(len(dataset)-15, len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    try:\n",
    "        # for updated datasets\n",
    "        undownsampled = dataset.get_undownsampled_item(i, standardizer, robust)\n",
    "    except AttributeError:\n",
    "        print('old dataset')\n",
    "        # for old datasets\n",
    "        undownsampled = None\n",
    "    mse = plot_autoencoding(sample, encoder, decoder, toss_encoder_output=False, \n",
    "                            reverse=TRAIN_REVERSED, undownsampled=undownsampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
